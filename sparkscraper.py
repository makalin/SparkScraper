import praw
import tweepy
import requests
from bs4 import BeautifulSoup
import time

# --- Reddit Setup ---
reddit = praw.Reddit(
    client_id="YOUR_REDDIT_CLIENT_ID",
    client_secret="YOUR_REDDIT_CLIENT_SECRET",
    user_agent="SparkScraper v1.0 by /u/YOUR_REDDIT_USERNAME"
)

# --- Twitter/X Setup ---
auth = tweepy.OAuth1UserHandler(
    consumer_key="YOUR_TWITTER_API_KEY",
    consumer_secret="YOUR_TWITTER_API_SECRET",
    access_token="YOUR_ACCESS_TOKEN",
    access_token_secret="YOUR_ACCESS_TOKEN_SECRET"
)
twitter_api = tweepy.API(auth)

# --- Functions to Scrape Data ---
def scrape_reddit(subreddit_name, keyword, limit=100):
    project_ideas = []
    subreddit = reddit.subreddit(subreddit_name)
    for submission in subreddit.search(keyword, limit=limit):
        if "project" in submission.title.lower() or "idea" in submission.title.lower():
            project_ideas.append(submission.title)
    return project_ideas

def scrape_twitter(keyword, count=100):
    project_ideas = []
    tweets = twitter_api.search_tweets(q=keyword, count=count, tweet_mode="extended")
    for tweet in tweets:
        text = tweet.full_text
        if "project" in text.lower() or "idea" in text.lower():
            project_ideas.append(text)
    return project_ideas

def scrape_linkedin(keyword):
    # Note: LinkedIn scraping is tricky due to login walls and ToS.
    url = f"https://www.linkedin.com/search/results/content/?keywords={keyword}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    posts = soup.find_all("div", class_="search-result__info")  # Adjust selector
    project_ideas = []
    for post in posts:
        text = post.get_text().strip()
        if "project" in text.lower() or "idea" in text.lower():
            project_ideas.append(text)
    return project_ideas

# --- Generate Markdown ---
def generate_markdown(reddit_ideas, twitter_ideas, linkedin_ideas):
    with open("sparkscraper_ideas.md", "w") as f:
        f.write("# SparkScraper Project Ideas\n\n")
        f.write("Generated by SparkScraper - Harvesting inspiration from the web!\n\n")
        
        f.write("## From Reddit\n")
        for i, idea in enumerate(reddit_ideas, 1):
            f.write(f"{i}. {idea}\n")
        
        f.write("\n## From Twitter/X\n")
        for i, idea in enumerate(twitter_ideas, 1):
            f.write(f"{i}. {idea}\n")
        
        f.write("\n## From LinkedIn\n")
        for i, idea in enumerate(linkedin_ideas, 1):
            f.write(f"{i}. {idea}\n")

# --- Main Execution ---
if __name__ == "__main__":
    keyword = "project ideas"  # Customize your search term
    subreddit = "sideprojects"  # Customize subreddit
    
    reddit_ideas = scrape_reddit(subreddit, keyword)
    twitter_ideas = scrape_twitter(keyword)
    linkedin_ideas = scrape_linkedin(keyword)  # Be cautious with this
    
    generate_markdown(reddit_ideas, twitter_ideas, linkedin_ideas)
    print("Project ideas saved to sparkscraper_ideas.md by SparkScraper!")